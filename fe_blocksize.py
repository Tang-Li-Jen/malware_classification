# author: Gert Jacobusse, gert.jacobusse@rogatio.nl
# licence: FreeBSD
# Copyright (c) 2015, Gert Jacobusse
# All rights reserved.

import os
import csv
import glob
import zipfile
from io import BytesIO
from collections import defaultdict
import re
import numpy as np
import pandas as pd

"""
function getcompressedsize_str
input: string
output: compressed size of string
* compress string and return the compressed size
"""
def getcompressedsize_str(strinput):
    inMemoryOutputFile = BytesIO()
    zf = zipfile.ZipFile(inMemoryOutputFile, 'w')
    zf.writestr('hello.csv',strinput, compress_type=zipfile.ZIP_DEFLATED)
    return zf.infolist()[0].compress_size

"""
function writeblocksizes
input: ids of trainset or testset, string "train" or "test"
output: writes train_blocksizes or test_blocksizes
* calculate and write compressed size of each 4 kB block for all files in train or test set
"""
def writeblocksizes(ids,trainortest,blocksize=256): # 256 times 16 bytes = 4096 bytes
    with open('%s_blocksizes.csv'%trainortest,'w') as fout:
        for i in ids:
            with open('%s/%s.bytes'%(trainortest,i),'r') as fin:
                contents=fin.readlines()
            # write md5
            fout.write('%s,'%i)
            n=len(contents)
            blocksize=256 
            nblock=n//256
            for b in range(nblock):
                strinput=''
                for lidx in range(b*blocksize,(b+1)*blocksize):
                    l=contents[lidx]
                    strinput += l[l.find(' ')+1:-1]
                s=getcompressedsize_str(strinput)
                fout.write('%d,'%s)
            fout.write('\n')

"""
function writeblocksizedistributions
input: string "train" or "test"
output: writes train_blocksizedistributions or test_blocksizedistributions
* calculate statistics on files with blocksizes to get the same number of features for each file
"""
def writeblocksizedistributions(trainortest):
    with open('%s_blocksizes.csv'%trainortest,'r') as f:
        with open('%s_blocksizedistributions.csv'%trainortest,'w') as fout:
            fout.write('cs4k_range,cs4k_std,cs4k_min,cs4k_p10,cs4k_p20,cs4k_p30,cs4k_p50,cs4k_p70,cs4k_p80,cs4k_p90,cs4k_max,cs4k_mean,cs4k_q1mean,cs4k_q2mean,cs4k_q3mean,cs4k_q4mean\n')
            for i,l in enumerate(f):
                ls=l.split(',')
                sizes=[float(e) for e in ls[1:-1]]
                slen=len(sizes)
                qlen=1 if slen // 4 <1 else slen // 4
                q1m=np.mean(sizes[:qlen])
                q2m=np.mean(sizes[qlen:2*qlen])
                q3m=np.mean(sizes[-2*qlen:-qlen])
                q4m=np.mean(sizes[-qlen:])
                sizes=sorted(sizes)
                maxidx=slen-1
                fout.write('%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f,%.0f\n'%(
                    sizes[-1] - sizes[0],
                    np.std(sizes),
                    sizes[0],
                    sizes[10*maxidx//100],
                    sizes[20*maxidx//100],
                    sizes[30*maxidx//100],
                    sizes[50*maxidx//100],
                    sizes[70*maxidx//100],
                    sizes[80*maxidx//100],
                    sizes[90*maxidx//100],
                    sizes[-1],
                    round(np.mean(sizes)),
                    q1m,q2m,q3m,q4m))


train_path = '/home/superroot/malware/train/'
train_data = glob.glob(train_path +'*.bytes')

train_path = '/home/superroot/malware/train/'
df_train = pd.read_csv('trainLabels.csv')
df_test = pd.read_csv('ResultSample.csv')

trainids = df_train['md5'].values
testids = df_test['md5'].values
labels = df_train['label'].values

assert len(trainids) == len(labels)
assert len(trainids) == 5907
assert len(testids) == 5000

writeblocksizes(trainids,'train')
writeblocksizes(testids,'test')

writeblocksizedistributions("train")
writeblocksizedistributions("test")