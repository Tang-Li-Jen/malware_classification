from collections import defaultdict
import operator
import gc
import pandas as pd

def convertfiletotokenarray(id_to_open, ngram, worddict):
    
    """

    Method to detect different bytes values and update the given dictionary

    The aim is to create a corpus with all unique words. 

    """   
    words_file = defaultdict(lambda: 0)
    fop = open(id_to_open.replace('"','') +'.bytes', 'r') # open the bytes file
    for mline in fop:
        if ngram <= 0: # take the whole line as gram
            mline = mline.replace("\n","")
            mline = mline[9:]
            if mline not in words_file:# if word not present in current dictionary...add the term of the word in  worddict
                words_file[mline] += 1
                worddict[mline] += 1                
        else: # use grams of single bytes
            split = mline.replace("\n","").split(" ") # break by space
            for ng in range(ngram-1, ngram):
                for j in range(ng+1, len(split)):
                    str_to_pass=""
                    for s in range (0, -ng-1, -1):
                        str_to_pass += split[j+s]
                    if str_to_pass not in words_file: # if word not present in current dictionary...add the term of the word in  worddict
                        words_file[str_to_pass] += 1
                        worddict[str_to_pass] += 1

                        

def convertfiletotokenarraybytetokens_new(id_to_open, ngram, indexesdict, files):
   
    """
    based on a dictionary of words and given ngrams, prints teh counts of bytes in the given file.
    
    """
    words_file = defaultdict(lambda: 0)
    fop = open(id_to_open.replace('"','') +'.bytes', 'r') # open the bytes file
    total_elements = 0
    for mline in fop:        
        split = mline.replace("\n","").split(" ")
        for ng in range(ngram-1,ngram):
            for j in range (ng + 1, len(split)):
                str_to_pass=""
                for s in range (0, -ng-1, -1): 
                    str_to_pass += split[j+s]
                if str_to_pass in indexesdict : # if word not present in current dictionary...add the term of the word in  worddict
                    words_file[str_to_pass] +=1
                    total_elements += 1

    files.write(str(id_to_open.split("/")[-1]) + "," + str(total_elements))
    for word, ind in indexesdict.items(): 
        if  word in words_file:
            bvalue = float(words_file[word])
            files.write("," + str(bvalue) )
        else:
            files.write(",0" )             
    files.write("\n")


def makesets_popularword(trainfile = "trainlabels.csv", testfile = "ResultSample.csv" ,  ngrams = 4,   output_name=""):
    
    """
    method that creates ngrams from bytes and includes 2 steps:

    1) scan teh files and the fine the bytes ("words")
    2) re-scan files and print new train and test sets with counts' distribution of the bytes' founds via the scanning process
    parameters:

    trainfile = the labels' file (becasuse we can get the ids)
    testfile = the sample submission file (becasuse we can get the ids for the tets cases)
    ngrams = number of ngrams. This considers the number of grands strictly. e.g ngrams=2 considers only ngrams 2 (not 1 grams too)
    output_name = prefix for the train and test files

    """
    
    #wordsother=loaddicts("E:/" + "dictspersubject"  , 1000000)
    words = defaultdict(lambda: 0)
    indices = defaultdict(lambda: 0)
    words_test = defaultdict(lambda: 0)

    print ("openning: " + str(trainfile))        
    tr = open (trainfile,'r') # open training labels' file that has the ids
    tr.readline() # headers        
    train_counter=0
    for line in tr : # for each line in the file
        splits = line.split(",")
        trid = splits[0] # e.g. a key
        
        convertfiletotokenarray(path + '/train/' + trid, ngrams, words)
        
        train_counter+=1
        if train_counter%500 == 0:
            print ("we are at train : " + str(train_counter)+ " length: " + str(len(words) ) )
        if len(words) > 5*10**7:
            print(" reached maximum number of cases: " + str(len(words)) )
            break
    
    tr.flush()
    tr.close()
    print(" finished training tf with total distinct words: " + str(len(words)))
    
    print("sorting...")
    
    word_sorted = sorted(words.items(), key = operator.itemgetter(1) , reverse = True) # reverse sort word by count  
    words = None
    
    gc.collect() # call garbage collector to release some memory
    
    
    # do the same with test as this time we work with samples and want to make certain that the words exist in both
    print ("openning: " + str(testfile) )
    te = open (testfile,'r') # open training labels' file taht has the ids
    te.readline() # headers       
    test_counter = 0
    
    for line in te : # for each line in the file
        splits = line.split(",")
        teid = splits[0] # e.g. a key

        convertfiletotokenarray(path + '/test/' + teid, ngrams, words_test)  

        test_counter += 1
        if test_counter%500 == 0:
            print ("we are at test : " + str(test_counter)+ " length: " + str(len(words_test) ) )
        
        if len(words_test) > 65536:
            print(" reached maximum number of cases: " + str(len(words_test)))               
            break
            
    te.flush()
    te.close()
                
    index = 2
    thress = 40000 # number of most popular ngrams to consider
    
    mini_c = 0
    for iu in range(0, len(word_sorted)):
        word = word_sorted[iu][0]
        if mini_c > thress:
            break
        if word in words_test: # if word is also in the test dictionary
            indices[word] = index
            index  += 1
            mini_c += 1
    

    #rest dictionaries
    words_test = None
    word_sorted = None

    gc.collect() # call garbage collector to release some memory
    
    
    ws = ",w_".join(indices.keys())
    
                        
    print (" max index is: " + str(index) )
    
    #create train set elements
    
    # Write Train Header
    trs = open(path + "/" + output_name + "_train.csv", "w")
    trs.write('md5' + ',' + str(ngrams) + 'gram_elements' + str(ws))
    trs.write("\n")
    
    
    print ("openning: " + str(trainfile) )        
    tr = open (trainfile,'r') # open training labels' file taht has the ids
    tr.readline() # headers        
    train_counter = 0
    
    for line in tr : # for each line in the file
        splits = line.split(",")
        trid = splits[0] # e.g. a key
        
        convertfiletotokenarraybytetokens_new(path + '/train/' + trid, ngrams, indices, trs) # print line in file   
        
        train_counter += 1
        if train_counter%500==0:
            print ("we are at train to write: " + str(train_counter))
            
    print ("create file with rows: " + str(train_counter))
    tr.flush()
    trs.flush()
    tr.close()
    trs.close()
    
    # Write Test header  
    tes = open(path + "/" + output_name + "_test.csv", "w")
    tes.write('md5' + ',' + str(ngrams) + '_total_elements' + str(ws))
    tes.write("\n")
    
    
    print ("openning: " + str(testfile) )    
    te = open (testfile,'r') # open training labels' file taht has the ids
    te.readline() # headers       
    test_counter = 0
    
    for line in te : # for each line in the file
        splits = line.split(",")
        teid = splits[0] # e.g. a key
        
        convertfiletotokenarraybytetokens_new(path + '/test/' + teid, ngrams, indices, tes) # print line in file    
        
        test_counter+=1
        if test_counter%500 == 0:
            print ("we are at test to write: " + str(test_counter))
    
    print ("create file with rows: " + str(test_counter) )
    te.flush()
    tes.flush()
    te.close()   
    tes.close()
    
    return indices

def makesets_coldword(trainfile = "trainlabels.csv", testfile = "ResultSample.csv" ,  ngrams = 2,   output_name=""):
    
    """
    method that creates ngrams from bytes and includes 2 steps:

    1) scan teh files and the fine the bytes ("words")
    2) re-scan files and print new train and test sets with counts' distribution of the bytes' founds via the scanning process
    parameters:

    trainfile = the labels' file (becasuse we can get the ids)
    testfile = the sample submission file (becasuse we can get the ids for the tets cases)
    ngrams = number of ngrams. This considers the number of grands strictly. e.g ngrams=2 considers only ngrams 2 (not 1 grams too)
    output_name = prefix for the train and test files

    """
    
    #wordsother=loaddicts("E:/" + "dictspersubject"  , 1000000)
    words = defaultdict(lambda: 0)
    indices = defaultdict(lambda: 0)
    words_test = defaultdict(lambda: 0)

    print ("openning: " + str(trainfile))        
    tr = open (trainfile,'r') # open training labels' file that has the ids
    tr.readline() # headers        
    train_counter=0
    for line in tr : # for each line in the file
        splits = line.split(",")
        trid = splits[0] # e.g. a key
        
        convertfiletotokenarray('train/' + trid, ngrams, words)
        
        train_counter+=1
        if train_counter%500 == 0:
            print ("we are at train : " + str(train_counter)+ " length: " + str(len(words) ) )
        if len(words) == 65536:
            print(" reached maximum number of cases: " + str(len(words)) )
            break
    
    tr.flush()
    tr.close()
    print(" finished training tf with total distinct words: " + str(len(words)))
    
    print("sorting...")
    
    word_sorted = sorted(words.items(), key = operator.itemgetter(1) , reverse = False) # reverse sort word by count  
    
    words = None
    
    gc.collect() # call garbage collector to release some memory
    
    
    # do the same with test as this time we work with samples and want to make certain that the words exist in both
    print ("openning: " + str(testfile) )
    te = open (testfile,'r') # open training labels' file taht has the ids
    te.readline() # headers       
    test_counter = 0
    
    for line in te : # for each line in the file
        splits = line.split(",")
        teid = splits[0] # e.g. a key

        convertfiletotokenarray('test/' + teid, ngrams, words_test)  

        test_counter += 1
        if test_counter%500 == 0:
            print ("we are at test : " + str(test_counter)+ " length: " + str(len(words_test) ) )
        
        if len(words_test) == 65536:
            print(" reached maximum number of cases: " + str(len(words_test)))               
            break
            
    te.flush()
    te.close()
                
    index = 2
    thress = 20000 # number of most popular ngrams to consider
    
    mini_c = 0
    for iu in range(0, len(word_sorted)):
        word = word_sorted[iu][0]
        if mini_c > thress:
            break
        if word in words_test: # if word is also in the test dictionary
            indices[word] = index
            index  += 1
            mini_c += 1

    #rest dictionaries
    words_test = None
    word_sorted = None
    gc.collect() # call garbage collector to release some memory
    
    
    ws = ",w_".join(indices.keys())
    
                        
    print (" max index is: " + str(index) )
    
    #create train set elements
    
    # Write Train Header
    trs = open("features/" + output_name + "_train.csv", "w")
    trs.write('md5' + ',' + str(ngrams) + 'gram_elements' + str(ws))
    trs.write("\n")
    
    
    print ("openning: " + str(trainfile) )        
    tr = open (trainfile,'r') # open training labels' file taht has the ids
    tr.readline() # headers        
    train_counter = 0
    
    for line in tr : # for each line in the file
        splits = line.split(",")
        trid = splits[0] # e.g. a key
        
        convertfiletotokenarraybytetokens_new('train/' + trid, ngrams, indices, trs) # print line in file   
        
        train_counter += 1
        if train_counter%500==0:
            print ("we are at train to write: " + str(train_counter))
            
    print ("create file with rows: " + str(train_counter))
    tr.flush()
    trs.flush()
    tr.close()
    trs.close()
    
    # Write Test header  
    tes = open("features/" + output_name + "_test.csv", "w")
    tes.write('md5' + ',' + str(ngrams) + '_total_elements' + str(ws))
    tes.write("\n")
    
    
    print ("openning: " + str(testfile) )    
    te = open (testfile,'r') # open training labels' file taht has the ids
    te.readline() # headers       
    test_counter = 0
    
    for line in te : # for each line in the file
        splits = line.split(",")
        teid = splits[0] # e.g. a key
        
        convertfiletotokenarraybytetokens_new('test/' + teid, ngrams, indices, tes) # print line in file    
        
        test_counter+=1
        if test_counter%500 == 0:
            print ("we are at test to write: " + str(test_counter))
    
    print ("create file with rows: " + str(test_counter) )
    te.flush()
    tes.flush()
    te.close()   
    tes.close()
    
    return indices

makesets_coldword(trainfile = "trainLabels.csv", testfile = "ResultSample.csv" ,\
                  ngrams = 4,   output_name="Invert_4gram")